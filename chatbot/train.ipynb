{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Initialize DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 16:14:57.501206: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [1,736101] vs. shape[1] = [1,332004]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ConcatV2_N_18_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [1,736101] vs. shape[1] = [1,332004] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     32\u001b[0m     attention_masks\u001b[38;5;241m.\u001b[39mappend(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 34\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(attention_masks, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/med-npc-nlp/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Downloads/med-npc-nlp/.venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ConcatV2_N_18_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [1,736101] vs. shape[1] = [1,332004] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory containing your text files\n",
    "data_directory = '../en'\n",
    "\n",
    "def load_data(directory):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):  \n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                doc_title = filename[:-4]  \n",
    "                doc_text = file.read()\n",
    "                documents[doc_title] = doc_text\n",
    "    return documents\n",
    "\n",
    "documents = load_data(data_directory)\n",
    "\n",
    "# Converting to data format needed\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for doc_title, doc_text in documents.items():\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        doc_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    input_ids.append(inputs['input_ids'])\n",
    "    attention_masks.append(inputs['attention_mask'])\n",
    "\n",
    "input_ids = tf.concat(input_ids, axis=0)\n",
    "attention_masks = tf.concat(attention_masks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query):\n",
    "    query_input = tokenizer.encode_plus(\n",
    "        query,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    query_output = model(\n",
    "        input_ids=query_input['input_ids'],\n",
    "        attention_mask=query_input['attention_mask']\n",
    "    )[0]  # Index to get the output logits\n",
    "    \n",
    "    # Compute cosine similarity between query and documents\n",
    "    doc_outputs = model(input_ids=input_ids, attention_mask=attention_masks)[0]\n",
    "    similarity_scores = tf.keras.losses.cosine_similarity(query_output, doc_outputs, axis=1)\n",
    "    \n",
    "    # Get the index of the most relevant document\n",
    "    retrieved_doc_index = tf.argmax(similarity_scores).numpy() # Convert tensor to integer\n",
    "    \n",
    "    # Convert the keys of documents to a list, then index with retrieved_doc_index\n",
    "    doc_keys = list(documents.keys())\n",
    "    return documents[doc_keys[retrieved_doc_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "gen_model = TFGPT2LMHeadModel.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    retrieved_doc = retrieve(query)\n",
    "    \n",
    "    # Combine query and retrieved document for generation input\n",
    "    generation_input = query + ' ' + retrieved_doc\n",
    "    \n",
    "    # Encode input for generation\n",
    "    gen_input = tokenizer.encode_plus(\n",
    "        generation_input,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,  # Adjusted to match the actual input length\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    # Generate a response\n",
    "    generated_response = gen_model.generate(\n",
    "        input_ids=gen_input['input_ids'],\n",
    "        attention_mask=gen_input['attention_mask'],\n",
    "        max_length=562,  # Adjusted to allow room for new tokens to be generated\n",
    "        num_beams=5,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        do_sample=True  # Added to allow sampling given temperature is set\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(generated_response[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "# Define your query\n",
    "query = \"What is the mechanism of action of Lisinopril?\"\n",
    "\n",
    "# Call the rag function with your query\n",
    "generated_response = rag(query)\n",
    "\n",
    "# Output the generated response\n",
    "print(generated_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
